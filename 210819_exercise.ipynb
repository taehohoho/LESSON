{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8136e351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Myexam/br_example_constitution.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile C:/Myexam/br_example_constitution.html\n",
    "<!doctype html>\n",
    "<html>\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <title> 줄바꿈 테스트 예제</title>\n",
    "    </head>\n",
    "    <body>\n",
    "    <p id=\"title\"><b>대한민국헌법</b></p>\n",
    "    <p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n",
    "    <p id=\"content\">제2조 <br/>①대한민국의 국민이 되는 요건은 법률로 정한다.<br/>②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.</p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ced8237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법\n",
      "제1조 ①대한민국은 민주공화국이다.②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n",
      "제2조 ①대한민국의 국민이 되는 요건은 법률로 정한다.②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "f = open('C:/Myexam/br_example_constitution.html',encoding='utf-8')\n",
    "\n",
    "html_source = f.read()\n",
    "f.close()\n",
    "\n",
    "soup = BeautifulSoup(html_source, \"lxml\")\n",
    "\n",
    "title = soup.find('p', {\"id\":\"title\"})\n",
    "contents = soup.find_all('p',{\"id\":\"content\"})\n",
    "\n",
    "print(title.get_text())\n",
    "for content in contents:\n",
    "    print(content.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "957ba33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_newline(soup_html):\n",
    "    br_to_newlines = soup_html.find_all('br')\n",
    "    for br_to_newline in br_to_newlines:\n",
    "        br_to_newline.replace_with(\"\\n\")\n",
    "    return soup_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7d111bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법 \n",
      "\n",
      "제1조 \n",
      "①대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다. \n",
      "\n",
      "제2조 \n",
      "①대한민국의 국민이 되는 요건은 법률로 정한다.\n",
      "②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html_source, \"lxml\")\n",
    "\n",
    "title = soup.find('p', {\"id\":\"title\"})\n",
    "contents = soup.find_all(\"p\", {\"id\":\"content\"})\n",
    "\n",
    "print(title.get_text(), \"\\n\")\n",
    "\n",
    "for content in contents :\n",
    "    content1 = replace_newline(content)\n",
    "    print(content1.get_text(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f10c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.alexa.com/topsites/countries/KR'\n",
    "\n",
    "html_website_ranking = requests.get(url).text\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, \"lxml\")\n",
    "\n",
    "# p태크의 요소 안에서 a 태그의 요소를 찾음\n",
    "website_ranking = soup_website_ranking.select('p a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abd0722e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://support.alexa.com/hc/en-us/articles/200444340\" target=\"_blank\">this explanation</a>,\n",
       " <a href=\"/siteinfo/google.com\">Google.com</a>,\n",
       " <a href=\"/siteinfo/naver.com\">Naver.com</a>,\n",
       " <a href=\"/siteinfo/youtube.com\">Youtube.com</a>,\n",
       " <a href=\"/siteinfo/daum.net\">Daum.net</a>,\n",
       " <a href=\"/siteinfo/tistory.com\">Tistory.com</a>,\n",
       " <a href=\"/siteinfo/kakao.com\">Kakao.com</a>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63779faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google.com'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[1].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8689f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_ranking_address = [website_ranking_element.get_text() for website_ranking_element in website_ranking [1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "287a2d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google.com',\n",
       " 'Naver.com',\n",
       " 'Youtube.com',\n",
       " 'Daum.net',\n",
       " 'Tistory.com',\n",
       " 'Kakao.com']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking_address[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0970393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Top Sites in South Korea]\n",
      "1:Google.com\n",
      "2:Naver.com\n",
      "3:Youtube.com\n",
      "4:Daum.net\n",
      "5:Tistory.com\n",
      "6:Kakao.com\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"\n",
    "\n",
    "html_website_ranking = requests.get(url).text\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, 'lxml')\n",
    "\n",
    "# p 태크의 요소 안에서 a 태그의 요소를 찾음\n",
    "website_ranking = soup_website_ranking.select('p a')\n",
    "website_ranking_address = [website_ranking_element.get_text()for website_ranking_element in website_ranking[1:]]\n",
    "\n",
    "print(\"[Top Sites in South Korea]\")\n",
    "for k in range(6):\n",
    "    print(\"{0}:{1}\".format((k+1), website_ranking_address[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbded7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naver.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daum.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tistory.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kakao.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       website\n",
       "1   Google.com\n",
       "2    Naver.com\n",
       "3  Youtube.com\n",
       "4     Daum.net\n",
       "5  Tistory.com\n",
       "6    Kakao.com"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "website_ranking_dict = {'website' : website_ranking_address}\n",
    "df = pd.DataFrame(website_ranking_dict, columns=['website'], index=range(1,len(website_ranking_address)+1))\n",
    "df[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "879964f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url : 'https://www.python.org/static/img/python-logo.png'\n",
    "html_image = requests.get(url)\n",
    "html_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cbef3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KR'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_file_name = os.path.basename(url)\n",
    "image_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "097d1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'C:/Myexam/download'\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a65e6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Myexam/download\\\\KR'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = os.path.join(folder, image_file_name)\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70ccee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile = open(image_path, 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ea26c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터를 1000000바이트씩 나눠서 내려받고 파일에 순차적으로 저장\n",
    "chunk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    ImageFile.write(chunk)\n",
    "ImageFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ff66049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KR']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfb2f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = 'https://www.python.org/static/img/python-logo.png'\n",
    "html_image = requests.get(url)\n",
    "image_file_name = os.path.basename(url)\n",
    "\n",
    "folder = 'C:/Myexam/download'\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "    \n",
    "image_path = os.path.join(folder, image_file_name)\n",
    "\n",
    "imagefile = open(image_path, 'wb')\n",
    "# 이미지 데이터를 1000000 바이트씩 나눠서 저장\n",
    "chunk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imagefile.write(chunk)\n",
    "imagefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46874bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"Reshot\" height=\"33\" src=\"https://www.reshot.com/build/reshot-logo--mark-cc49363ac3f7876f854286af4266ead51a7ff9e0fa12f30677c9e758d43dd0d1.svg\" title=\"Reshot\" width=\"46\"/>,\n",
       " <img alt=\"\" class=\"photos-item-card__image\" height=\"2448\" loading=\"lazy\" src=\"https://res.cloudinary.com/twenty20/private_images/t_reshot-400/v1521838685/photosp/bae96789-a5ab-4471-b54f-9686ace09e33/bae96789-a5ab-4471-b54f-9686ace09e33.jpg\" width=\"2448\"/>,\n",
       " <img alt=\"Back off!\" class=\"photos-item-card__image\" height=\"2361\" loading=\"lazy\" src=\"https://res.cloudinary.com/twenty20/private_images/t_reshot-400/v1597098233/photosp/a44357c5-b1c3-41ef-9a65-7a4937b06a44/a44357c5-b1c3-41ef-9a65-7a4937b06a44.jpg\" width=\"3148\"/>,\n",
       " <img alt=\"Orphans\" class=\"photos-item-card__image\" height=\"3375\" loading=\"lazy\" src=\"https://res.cloudinary.com/twenty20/private_images/t_reshot-400/v1617578418/photosp/34fd9c70-8996-4706-a0f1-113231ed3eee/34fd9c70-8996-4706-a0f1-113231ed3eee.jpg\" width=\"3375\"/>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.reshot.com/free-stock-photos/search/animal'\n",
    "\n",
    "html_reshot_image = requests.get(url).text\n",
    "soup_reshot_image = BeautifulSoup(html_reshot_image, 'lxml')\n",
    "reshot_image_elements = soup_reshot_image.select('a img')\n",
    "reshot_image_elements[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f757dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://res.cloudinary.com/twenty20/private_images/t_reshot-400/v1521838685/photosp/bae96789-a5ab-4471-b54f-9686ace09e33/bae96789-a5ab-4471-b54f-9686ace09e33.jpg'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshot_image_url = reshot_image_elements[1].get('src')\n",
    "reshot_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "503f763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_image = requests.get(reshot_image_url)\n",
    "\n",
    "folder = \"C:/Myexam/download\"\n",
    "\n",
    "# os.path.basename(url)는 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리하는 방법\n",
    "imagefile = open(os.path.join(folder, os.path.basename(reshot_image_url)), 'wb')\n",
    "\n",
    "# 이미지 데이터를 10000000 바이트씩 나눠서 저장하는 방법\n",
    "chunk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imagefile.write(chunk)\n",
    "imagefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4deba9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 파일명:\"reshot-logo--mark-cc49363ac3f7876f854286af4266ead51a7ff9e0fa12f30677c9e758d43dd0d1.svg\". 내려받기 완료! \n",
      "이미지 파일명:\"bae96789-a5ab-4471-b54f-9686ace09e33.jpg\". 내려받기 완료! \n",
      "이미지 파일명:\"a44357c5-b1c3-41ef-9a65-7a4937b06a44.jpg\". 내려받기 완료! \n",
      "이미지 파일명:\"34fd9c70-8996-4706-a0f1-113231ed3eee.jpg\". 내려받기 완료! \n",
      "이미지 파일명:\"dbd9fa3b-238b-47b1-8e20-c05400cbe921.jpg\". 내려받기 완료! \n",
      "이미지 파일명:\"737d192f-ba38-4a71-9bb9-9d40b45d0263.jpg\". 내려받기 완료! \n",
      "이미지 파일명:\"c3c3604d-36eb-4f8a-9768-cebc0749d5a5.jpg\". 내려받기 완료! \n",
      "===============================\n",
      "선택한 모든 이미지 내려받기 완료!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# url(주소)에서 이미지 주소 추출\n",
    "def get_image_url(url):\n",
    "    html_image_url = requests.get(url).text\n",
    "    soup_image_url = BeautifulSoup(html_image_url, \"lxml\")\n",
    "    image_elements = soup_image_url.select('img')\n",
    "    if(image_elements != None):\n",
    "        image_urls = []\n",
    "        for image_element in image_elements:\n",
    "            image_urls.append(image_element.get('src'))\n",
    "        return image_urls\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# 폴더를 지정해 이미지 주소에서 이미지 내려받기\n",
    "def download_image(img_folder, img_url):\n",
    "    if(img_url != None):\n",
    "        html_image = requests.get(img_url)\n",
    "        # os.path.basename(url)는 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리\n",
    "        imagefile = open(os.path.join(img_folder, os.path.basename(img_url)),'wb')\n",
    "        chunk_size = 1000000\n",
    "        for chunk in html_image.iter_content(chunk_size):\n",
    "            imagefile.write(chunk)\n",
    "            imagefile.close()\n",
    "        print('이미지 파일명:\"{0}\". 내려받기 완료! '.format(os.path.basename(img_url)))\n",
    "    else:\n",
    "        print(\"내려받을 이미지가 없습니다.\")\n",
    "        \n",
    "# 웹사이트의 주소 지정\n",
    "reshot_url = 'http://www.reshot.com/search/animal'\n",
    "\n",
    "figure_folder = \"C:/Myexam/download\" # 이미지를 내려받을 폴더 지정\n",
    "\n",
    "reshot_image_urls = get_image_url(reshot_url) # 이미지 파일의 주소 가져오기\n",
    "\n",
    "num_of_download_image = 7 # 내려받을 이미지 개수 지정\n",
    "# num_of_download_image = len(reshot_image_urls)\n",
    "\n",
    "for k in range(num_of_download_image):\n",
    "    download_image(figure_folder, reshot_image_urls[k])\n",
    "print(\"===============================\")\n",
    "print(\"선택한 모든 이미지 내려받기 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c46fd4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_download_image = len(reshot_image_urls)\n",
    "num_of_download_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
